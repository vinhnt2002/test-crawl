# Etsy Scraper using ScraperAPI

Há»‡ thá»‘ng crawl dá»¯ liá»‡u Etsy sá»­ dá»¥ng ScraperAPI Ä‘á»ƒ trÃ¡nh bá»‹ block vÃ  cÃ³ hiá»‡u suáº¥t cao.

## ğŸš€ TÃ­nh nÄƒng

- âœ… TÃ¬m kiáº¿m sáº£n pháº©m theo tá»« khÃ³a
- âœ… Crawl chi tiáº¿t sáº£n pháº©m
- âœ… Crawl sáº£n pháº©m trending
- âœ… Crawl nhiá»u tá»« khÃ³a cÃ¹ng lÃºc
- âœ… LÆ°u dá»¯ liá»‡u vÃ o MongoDB
- âœ… Retry mechanism vÃ  error handling
- âœ… Rate limiting Ä‘á»ƒ trÃ¡nh bá»‹ ban
- âœ… Support proxy vÃ  rendering JS

## ğŸ“¦ CÃ i Ä‘áº·t

```bash
# CÃ i Ä‘áº·t dependencies
npm install node-fetch cheerio dotenv

# Hoáº·c náº¿u chÆ°a cÃ³ trong package.json
npm install node-fetch@2.6.7 cheerio@1.0.0-rc.12 dotenv
```

## âš™ï¸ Cáº¥u hÃ¬nh

1. **Táº¡o file `.env`** trong thÆ° má»¥c gá»‘c:
```env
SCRAPERAPI_KEY=f7e71afbac6f3cb929b51fe6300e3045
MONGODB_URI=your_mongodb_connection_string
DATABASE_NAME=etsyData
```

2. **Láº¥y API Key tá»« ScraperAPI:**
   - ÄÄƒng kÃ½ táº¡i: https://www.scraperapi.com/
   - Free plan: 1000 requests/month
   - Paid plans: nhiá»u features vÃ  requests hÆ¡n

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### 1. Sá»­ dá»¥ng cÆ¡ báº£n

```javascript
const EtsyScraper = require('./ScraperAPI/etsy-scraper');

async function main() {
  const scraper = new EtsyScraper();
  
  // TÃ¬m kiáº¿m sáº£n pháº©m
  const products = await scraper.searchEtsyProducts('handmade jewelry', 1, 20);
  console.log(`Found ${products.length} products`);
  
  // LÆ°u vÃ o database
  await scraper.saveProducts(products);
}

main();
```

### 2. Crawl nhiá»u tá»« khÃ³a

```javascript
const scraper = new EtsyScraper();

const keywords = [
  'vintage rings',
  'handmade earrings',
  'custom necklace'
];

const allProducts = await scraper.crawlMultipleKeywords(keywords, 3);
console.log(`Total: ${allProducts.length} products`);
```

### 3. Crawl trending products

```javascript
const scraper = new EtsyScraper();

// Crawl trending theo category
const jewelry = await scraper.crawlTrendingProducts('jewelry');
const allTrending = await scraper.crawlTrendingProducts(); // All categories
```

### 4. Crawl chi tiáº¿t sáº£n pháº©m

```javascript
const productUrl = 'https://www.etsy.com/listing/123456789/example-product';
const details = await scraper.getProductDetails(productUrl);
console.log(details);
```

## ğŸ”§ Configuration

Xem file `config.js` Ä‘á»ƒ tÃ¹y chá»‰nh:

- **Delays**: Thá»i gian nghá»‰ giá»¯a cÃ¡c requests
- **Selectors**: CSS selectors Ä‘á»ƒ parse dá»¯ liá»‡u
- **Categories**: Danh má»¥c Etsy
- **Keywords**: Tá»« khÃ³a phá»• biáº¿n

## ğŸ“Š Dá»¯ liá»‡u thu tháº­p

### Product Data Structure
```javascript
{
  product_id: "123456789",
  title: "Handmade Silver Ring",
  price: "$45.00",
  shop_name: "ArtisanJewelry",
  image_url: "https://...",
  product_url: "https://www.etsy.com/listing/...",
  rating: "4.8 stars",
  keyword_searched: "handmade jewelry",
  scraped_at: new Date(),
  source: "etsy"
}
```

### Detailed Product Data
```javascript
{
  // ... basic data ...
  description: "Beautiful handcrafted ring...",
  images: ["url1", "url2", "url3"],
  tags: ["handmade", "jewelry", "ring"],
  category: "Jewelry",
  reviews_count: "150 reviews",
  shop_url: "/shop/ArtisanJewelry"
}
```

## ğŸš€ Cháº¡y Demo

```bash
node ScraperAPI/demo-etsy.js
```

## ğŸ“ VÃ­ dá»¥ Scripts

### Script 1: Crawl by Keywords
```javascript
// crawl-keywords.js
const EtsyScraper = require('./ScraperAPI/etsy-scraper');
const config = require('./ScraperAPI/config');

async function crawlKeywords() {
  const scraper = new EtsyScraper();
  
  // Sá»­ dá»¥ng keywords tá»« config
  const results = await scraper.crawlMultipleKeywords(
    config.popularKeywords.slice(0, 5), // Chá»‰ crawl 5 keywords Ä‘áº§u
    2 // 2 pages per keyword
  );
  
  console.log(`âœ… Crawled ${results.length} products`);
}

crawlKeywords();
```

### Script 2: Monitor Trending
```javascript
// monitor-trending.js
const EtsyScraper = require('./ScraperAPI/etsy-scraper');

async function monitorTrending() {
  const scraper = new EtsyScraper();
  
  const categories = ['jewelry', 'clothing', 'home', 'art'];
  
  for (const category of categories) {
    console.log(`\nğŸ“ˆ Crawling trending: ${category}`);
    const products = await scraper.crawlTrendingProducts(category);
    console.log(`Found ${products.length} trending products in ${category}`);
    
    await scraper.delay(5000); // 5s delay between categories
  }
}

monitorTrending();
```

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **Rate Limiting**: LuÃ´n cÃ³ delay giá»¯a cÃ¡c requests
2. **ScraperAPI Credits**: Theo dÃµi usage Ä‘á»ƒ khÃ´ng vÆ°á»£t quÃ¡ limit
3. **Selectors**: Etsy cÃ³ thá»ƒ thay Ä‘á»•i HTML structure, cáº§n update selectors
4. **Error Handling**: LuÃ´n cÃ³ try-catch cho cÃ¡c network requests
5. **Data Quality**: Validate dá»¯ liá»‡u trÆ°á»›c khi lÆ°u vÃ o database

## ğŸ› ï¸ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p:

1. **403/429 Errors**: 
   - TÄƒng delay giá»¯a requests
   - Sá»­ dá»¥ng rotating proxies
   - Check ScraperAPI credit

2. **Parsing Errors**:
   - Etsy Ä‘Ã£ thay Ä‘á»•i HTML structure
   - Update selectors trong config.js

3. **Database Errors**:
   - Check MongoDB connection
   - Verify collection permissions

## ğŸ“ˆ Performance Tips

1. **Parallel Processing**: Crawl nhiá»u keywords song song (cáº©n tháº­n vá»›i rate limit)
2. **Caching**: Cache káº¿t quáº£ Ä‘á»ƒ trÃ¡nh crawl láº·p
3. **Incremental Updates**: Chá»‰ crawl sáº£n pháº©m má»›i
4. **Data Compression**: Compress large text fields

## ğŸ’¡ Ã tÆ°á»Ÿng má»Ÿ rá»™ng

- ğŸ”„ Scheduled crawling vá»›i cron jobs
- ğŸ“Š Analytics vÃ  reporting dashboard  
- ğŸ”” Alert system cho trending products
- ğŸ¯ Price monitoring vÃ  tracking
- ğŸ” Advanced search filters
- ğŸ“± API endpoint cho frontend
- ğŸ¤– AI-powered product categorization

## ğŸ†˜ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Check logs Ä‘á»ƒ identify error
2. Verify ScraperAPI account status
3. Test vá»›i single product trÆ°á»›c
4. Check Etsy website changes

---

**Happy Scraping! ğŸ‰** 